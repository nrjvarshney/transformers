{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python36.zip', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6/lib-dynload', '', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6/site-packages', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6/site-packages/IPython/extensions', '/home/nvarshn2/.ipython']\n",
      "--------\n",
      "['/home/nvarshn2/Neeraj/transformers/src/transformers', '/home/nvarshn2/Neeraj/transformers/src', '/home/nvarshn2/Neeraj/transformers', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python36.zip', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6/lib-dynload', '', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6/site-packages', '/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6/site-packages/IPython/extensions', '/home/nvarshn2/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.insert(0,\"/home/nvarshn2/Neeraj/transformers\")\n",
    "sys.path.insert(0,\"/home/nvarshn2/Neeraj/transformers/src\")\n",
    "sys.path.insert(0,\"/home/nvarshn2/Neeraj/transformers/src/transformers\")\n",
    "print(\"--------\")\n",
    "print(sys.path)\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMultipleChoice,\n",
    "    BertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForMultipleChoice_Custom3,\n",
    "    XLNetConfig,\n",
    "    XLNetForMultipleChoice,\n",
    "    XLNetTokenizer,\n",
    "    AlbertConfig,\n",
    "    AlbertForMultipleChoice,\n",
    "    AlbertTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "from utils_multiple_choice import convert_examples_to_features, processors\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ALL_MODELS = sum(\n",
    "    (tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig, XLNetConfig, RobertaConfig)), ()\n",
    ")\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bert\": (BertConfig, BertForMultipleChoice, BertTokenizer),\n",
    "    \"xlnet\": (XLNetConfig, XLNetForMultipleChoice, XLNetTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMultipleChoice_Custom3, RobertaTokenizer),\n",
    "    \"albert\": (AlbertConfig, AlbertForMultipleChoice, AlbertTokenizer),\n",
    "}\n",
    "\n",
    "\n",
    "def select_field(features, field):\n",
    "    return [[choice[field] for choice in feature.choices_features] for feature in features]\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    best_dev_acc = 0.0\n",
    "    best_steps = 0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    set_seed(args)  # Added here for reproductibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2]\n",
    "                if args.model_type in [\"bert\", \"xlnet\"]\n",
    "                else None,  # XLM don't use segment_ids\n",
    "                \"labels\": batch[3],\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                        if results[\"eval_acc\"] > best_dev_acc:\n",
    "                            best_dev_acc = results[\"eval_acc\"]\n",
    "                            best_steps = global_step\n",
    "                            if args.do_test:\n",
    "                                results_test = evaluate(args, model, tokenizer, test=True)\n",
    "                                for key, value in results_test.items():\n",
    "                                    tb_writer.add_scalar(\"test_{}\".format(key), value, global_step)\n",
    "                                logger.info(\n",
    "                                    \"test acc: %s, loss: %s, global steps: %s\",\n",
    "                                    str(results_test[\"eval_acc\"]),\n",
    "                                    str(results_test[\"eval_loss\"]),\n",
    "                                    str(global_step),\n",
    "                                )\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logger.info(\n",
    "                        \"Average loss: %s at global step: %s\",\n",
    "                        str((tr_loss - logging_loss) / args.logging_steps),\n",
    "                        str(global_step),\n",
    "                    )\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_vocabulary(output_dir)\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step, best_steps\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(args, model, tokenizer, prefix=\"\", test=False):\n",
    "    eval_task_names = (args.task_name,)\n",
    "    eval_outputs_dirs = (args.output_dir,)\n",
    "    results = {}\n",
    "    CLS_representations = []\n",
    "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
    "        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=not test, test=test)\n",
    "\n",
    "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(eval_output_dir)\n",
    "\n",
    "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        eval_sampler = SequentialSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        # multi-gpu evaluate\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        output_logit_list = []\n",
    "        out_label_ids = None\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            model.eval()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"token_type_ids\": batch[2]\n",
    "                    if args.model_type in [\"bert\", \"xlnet\"]\n",
    "                    else None,  # XLM don't use segment_ids\n",
    "                    \"labels\": batch[3],\n",
    "                }\n",
    "                outputs = model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "#                 pooled_outputs = outputs[2]\n",
    "\n",
    "#                 import time\n",
    "#                 time.sleep(1000)\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "            output_logit_list += logits\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        acc = simple_accuracy(preds, out_label_ids)\n",
    "        result = {\"eval_acc\": acc, \"eval_loss\": eval_loss}\n",
    "        results.update(result)\n",
    "\n",
    "        # uncomment the next block to write the output to file\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"is_test_\" + str(test).lower() + \"_eval_results.txt\")\n",
    "\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results {} *****\".format(str(prefix) + \" is test:\" + str(test)))\n",
    "            writer.write(\"model           =%s\\n\" % str(args.model_name_or_path))\n",
    "            writer.write(\n",
    "                \"total batch size=%d\\n\"\n",
    "                % (\n",
    "                    args.per_gpu_train_batch_size\n",
    "                    * args.gradient_accumulation_steps\n",
    "                    * (torch.distributed.get_world_size() if args.local_rank != -1 else 1)\n",
    "                )\n",
    "            )\n",
    "            writer.write(\"train num epochs=%d\\n\" % args.num_train_epochs)\n",
    "            writer.write(\"fp16            =%s\\n\" % args.fp16)\n",
    "            writer.write(\"max seq length  =%d\\n\" % args.max_seq_length)\n",
    "            for ele in output_logit_list:\n",
    "                writer.write(\"%s\\n\" % ele)\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def load_and_cache_examples(args, task, tokenizer, evaluate=False, test=False):\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    processor = processors[task]()\n",
    "    # Load data features from cache or dataset file\n",
    "    if evaluate:\n",
    "        cached_mode = \"dev\"\n",
    "    elif test:\n",
    "        cached_mode = \"test\"\n",
    "    else:\n",
    "        cached_mode = \"train\"\n",
    "    assert not (evaluate and test)\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}_{}\".format(\n",
    "            cached_mode,\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(args.max_seq_length),\n",
    "            str(task),\n",
    "        ),\n",
    "    )\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        if evaluate:\n",
    "            examples = processor.get_dev_examples(args.data_dir)\n",
    "        elif test:\n",
    "            examples = processor.get_test_examples(args.data_dir)\n",
    "        else:\n",
    "            examples = processor.get_train_examples(args.data_dir)\n",
    "        logger.info(\"Training number: %s\", str(len(examples)))\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            label_list,\n",
    "            args.max_seq_length,\n",
    "            tokenizer,\n",
    "            pad_on_left=bool(args.model_type in [\"xlnet\"]),  # pad on the left for xlnet\n",
    "            pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
    "        )\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor(select_field(features, \"input_ids\"), dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(select_field(features, \"input_mask\"), dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(select_field(features, \"segment_ids\"), dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/04/2020 08:13:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: False\n",
      "04/04/2020 08:13:09 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-config.json from cache at /home/nvarshn2/.cache/torch/transformers/b3eed512e24335a76694282193217608ead013caa55330de3ff236d1f5695e6c.58c1d03602d9707494c5ff902d07817fd2b4ed6a589b2b062364aed4ae3d3765\n",
      "04/04/2020 08:13:09 - INFO - transformers.configuration_utils -   Model config AlbertConfig {\n",
      "  \"_num_labels\": 5,\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": null,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"do_sample\": false,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": \"csqa\",\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layers_to_keep\": [],\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "04/04/2020 08:13:10 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-spiece.model from cache at /home/nvarshn2/.cache/torch/transformers/094b3b4d4ab5e624ae6ba8654d88cdb99b2d5a813b323295c37d58680a1c4127.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf\n",
      "04/04/2020 08:13:10 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/albert-xxlarge-v2-pytorch_model.bin from cache at /home/nvarshn2/.cache/torch/transformers/c8f990f22da3ddf461b7e0d30a079014b20ad2859f352a9f18421485f63a69e7.9ac42d6fae7d18840d74eaf2a6d817700ffdd5af9ae1a12c3e96e239e23f76f4\n",
      "04/04/2020 08:13:17 - INFO - transformers.modeling_utils -   Weights of AlbertForMultipleChoice not initialized from pretrained model: ['classifier2.weight', 'classifier2.bias']\n",
      "04/04/2020 08:13:17 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in AlbertForMultipleChoice: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "04/04/2020 08:13:21 - INFO - __main__ -   Training/evaluation parameters {'model_type': 'albert', 'task_name': 'csqa', 'model_name_or_path': 'albert-xxlarge-v2', 'do_eval': True, 'do_test': False, 'do_train': True, 'do_lower_case': True, 'data_dir': './common_sense_data/', 'per_gpu_eval_batch_size': 2, 'overwrite_output': True, 'local_rank': -1, 'server_ip': '', 'server_port': '', 'fp16_opt_level': 'O1', 'fp16': False, 'seed': 42, 'overwrite_cache': False, 'overwrite_output_dir': True, 'no_cuda': False, 'eval_all_checkpoints': False, 'save_steps': 500, 'logging_steps': 500, 'warmup_steps': 608, 'max_steps': -1, 'num_train_epochs': 4, 'max_grad_norm': 1.0, 'adam_epsilon': 1e-08, 'weight_decay': 0.0, 'learning_rate': 1e-05, 'gradient_accumulation_steps': 4, 'per_gpu_train_batch_size': 2, 'evaluate_during_training': False, 'max_seq_length': 80, 'cache_dir': '', 'tokenizer_name': '', 'config_name': '', 'output_dir': './csqa_result', 'n_gpu': 2, 'device': device(type='cuda')}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = {\n",
    "    \"model_type\": None,\n",
    "    \"task_name\": \"csqa\",\n",
    "    \"model_name_or_path\" : None,\n",
    "    \"do_eval\": False,\n",
    "    \"do_test\": False,\n",
    "    \"do_train\": False,\n",
    "    \"do_lower_case\": False,\n",
    "    \"data_dir\": None,\n",
    "    \"per_gpu_eval_batch_size\" : 16,\n",
    "    \"overwrite_output\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"server_ip\": \"\",\n",
    "    \"server_port\": \"\",\n",
    "    \"fp16_opt_level\": \"O1\",\n",
    "    \"fp16\": False,\n",
    "    \"seed\": 42,\n",
    "    \"overwrite_cache\": False,\n",
    "    \"overwrite_output_dir\": False,\n",
    "    \"no_cuda\": False,\n",
    "    \"eval_all_checkpoints\": False,\n",
    "    \"save_steps\": 500,\n",
    "    \"logging_steps\": 500,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"max_steps\": -1,\n",
    "    \"num_train_epochs\": 3.0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"per_gpu_eval_batch_size\": 8,\n",
    "    \"per_gpu_train_batch_size\": 8,\n",
    "    \"evaluate_during_training\": False,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"cache_dir\": \"\",\n",
    "    \"tokenizer_name\": \"\",\n",
    "    \"config_name\": \"\",\n",
    "    \"output_dir\": None,\n",
    "}\n",
    "\n",
    "args[\"model_type\"] = \"albert\"\n",
    "args[\"task_name\"] = \"csqa\"\n",
    "# args[\"model_name_or_path\"] = \"./csqa_roberta_large_result/\"\n",
    "args[\"model_name_or_path\"] = \"albert-xxlarge-v2\"\n",
    "args[\"do_eval\"] = True\n",
    "args[\"do_train\"] = True\n",
    "args[\"do_lower_case\"] = True\n",
    "\n",
    "args[\"max_seq_length\"] = 80\n",
    "args[\"learning_rate\"] = 1e-5\n",
    "args[\"num_train_epochs\"] = 4\n",
    "args[\"data_dir\"] = \"./common_sense_data/\"\n",
    "args[\"output_dir\"] = \"./csqa_result\"\n",
    "args[\"per_gpu_eval_batch_size\"] = 2\n",
    "args[\"per_gpu_train_batch_size\"] = 2\n",
    "args[\"gradient_accumulation_steps\"] = 4\n",
    "args[\"overwrite_output\"] = True\n",
    "args[\"overwrite_output_dir\"] = True\n",
    "\n",
    "args[\"warmup_steps\"] = 608\n",
    "\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "temp_args = AttrDict()\n",
    "temp_args.update(args)\n",
    "args = temp_args\n",
    "\n",
    "if (\n",
    "    os.path.exists(args.output_dir)\n",
    "    and os.listdir(args.output_dir)\n",
    "    and args.do_train\n",
    "    and not args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Setup distant debugging if needed\n",
    "if args.server_ip and args.server_port:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "logger.warning(\n",
    "    \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "    args.local_rank,\n",
    "    device,\n",
    "    args.n_gpu,\n",
    "    bool(args.local_rank != -1),\n",
    "    args.fp16,\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Prepare GLUE task\n",
    "args.task_name = args.task_name.lower()\n",
    "if args.task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "processor = processors[args.task_name]()\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=args.task_name,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    output_hidden_states = True,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "    do_lower_case=args.do_lower_case,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    \n",
    ")\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "best_steps = 0\n",
    "\n",
    "\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/04/2020 08:13:21 - INFO - __main__ -   Loading features from cached file ./common_sense_data/cached_train_albert-xxlarge-v2_80_csqa\n",
      "04/04/2020 08:13:22 - INFO - __main__ -   ***** Running training *****\n",
      "04/04/2020 08:13:22 - INFO - __main__ -     Num examples = 9741\n",
      "04/04/2020 08:13:22 - INFO - __main__ -     Num Epochs = 4\n",
      "04/04/2020 08:13:22 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
      "04/04/2020 08:13:22 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "04/04/2020 08:13:22 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
      "04/04/2020 08:13:22 - INFO - __main__ -     Total optimization steps = 2436\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/2436 [00:00<?, ?it/s]\u001b[A/home/nvarshn2/.conda/envs/transformers_nrj_env/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "Iteration:   0%|          | 1/2436 [00:04<3:09:39,  4.67s/it]\u001b[A\n",
      "Iteration:   0%|          | 2/2436 [00:05<2:25:09,  3.58s/it]\u001b[A\n",
      "Iteration:   0%|          | 3/2436 [00:06<1:54:06,  2.81s/it]\u001b[A\n",
      "Iteration:   0%|          | 4/2436 [00:07<1:32:26,  2.28s/it]\u001b[A\n",
      "Iteration:   0%|          | 5/2436 [00:08<1:17:16,  1.91s/it]\u001b[A\n",
      "Iteration:   0%|          | 6/2436 [00:09<1:06:26,  1.64s/it]\u001b[A\n",
      "Iteration:   0%|          | 7/2436 [00:10<58:55,  1.46s/it]  \u001b[A\n",
      "Iteration:   0%|          | 8/2436 [00:11<53:50,  1.33s/it]\u001b[A\n",
      "Iteration:   0%|          | 9/2436 [00:12<50:21,  1.25s/it]\u001b[A\n",
      "Iteration:   0%|          | 10/2436 [00:13<47:50,  1.18s/it]\u001b[A\n",
      "Iteration:   0%|          | 11/2436 [00:14<45:57,  1.14s/it]\u001b[A\n",
      "Iteration:   0%|          | 12/2436 [00:16<44:34,  1.10s/it]\u001b[A\n",
      "Iteration:   1%|          | 13/2436 [00:17<43:48,  1.08s/it]\u001b[A\n",
      "Iteration:   1%|          | 14/2436 [00:18<43:08,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|          | 15/2436 [00:19<42:39,  1.06s/it]\u001b[A\n",
      "Iteration:   1%|          | 16/2436 [00:20<42:24,  1.05s/it]\u001b[A\n",
      "Iteration:   1%|          | 17/2436 [00:21<42:17,  1.05s/it]\u001b[A\n",
      "Iteration:   1%|          | 18/2436 [00:22<42:01,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 19/2436 [00:23<41:48,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 20/2436 [00:24<41:50,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 21/2436 [00:25<41:55,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 22/2436 [00:26<41:52,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 23/2436 [00:27<41:47,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 24/2436 [00:28<41:47,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 25/2436 [00:29<41:55,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 26/2436 [00:30<41:52,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 27/2436 [00:31<41:45,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 28/2436 [00:32<41:42,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 29/2436 [00:33<41:49,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|          | 30/2436 [00:34<41:42,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 31/2436 [00:35<41:42,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 32/2436 [00:36<41:42,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 33/2436 [00:37<41:46,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 34/2436 [00:38<41:46,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 35/2436 [00:39<41:46,  1.04s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 36/2436 [00:40<41:46,  1.04s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 37/2436 [00:42<41:55,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 38/2436 [00:43<41:50,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 39/2436 [00:44<41:46,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 40/2436 [00:45<41:44,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 41/2436 [00:46<41:52,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 42/2436 [00:47<41:44,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 43/2436 [00:48<41:39,  1.04s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 44/2436 [00:49<41:38,  1.04s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 45/2436 [00:50<41:48,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 46/2436 [00:51<41:41,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 47/2436 [00:52<41:40,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 48/2436 [00:53<41:39,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 49/2436 [00:54<41:49,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 50/2436 [00:55<41:39,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 51/2436 [00:56<41:30,  1.04s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 52/2436 [00:57<41:28,  1.04s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 53/2436 [00:58<41:37,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 54/2436 [00:59<41:38,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 55/2436 [01:00<41:32,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 56/2436 [01:01<41:33,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 57/2436 [01:02<41:42,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 58/2436 [01:04<41:37,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 59/2436 [01:05<41:33,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 60/2436 [01:06<41:32,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 61/2436 [01:07<41:39,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 62/2436 [01:08<41:35,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 63/2436 [01:09<41:37,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 64/2436 [01:10<41:46,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 65/2436 [01:11<41:49,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 66/2436 [01:12<41:44,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 67/2436 [01:13<41:35,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 68/2436 [01:14<41:31,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 69/2436 [01:15<41:36,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 70/2436 [01:16<41:29,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 71/2436 [01:17<41:21,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 72/2436 [01:18<41:24,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 73/2436 [01:19<41:29,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 74/2436 [01:20<41:23,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 75/2436 [01:21<41:17,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 76/2436 [01:22<41:21,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 77/2436 [01:24<41:25,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 78/2436 [01:25<41:23,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 79/2436 [01:26<41:26,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 80/2436 [01:27<41:24,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 81/2436 [01:28<41:31,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 82/2436 [01:29<41:31,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 83/2436 [01:30<41:30,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 84/2436 [01:31<41:26,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 85/2436 [01:32<41:31,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 86/2436 [01:33<41:24,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 87/2436 [01:34<41:20,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 88/2436 [01:35<41:19,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 89/2436 [01:36<41:25,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 90/2436 [01:37<41:19,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 91/2436 [01:38<41:18,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 92/2436 [01:39<41:24,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 93/2436 [01:40<41:27,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 94/2436 [01:42<41:21,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 95/2436 [01:43<41:10,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 96/2436 [01:44<41:19,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 97/2436 [01:45<41:26,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 98/2436 [01:46<41:16,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 99/2436 [01:47<41:12,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 100/2436 [01:48<41:10,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 101/2436 [01:49<41:14,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 102/2436 [01:50<41:08,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 103/2436 [01:51<41:04,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 104/2436 [01:52<41:04,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 105/2436 [01:53<41:08,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 106/2436 [01:54<41:04,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 107/2436 [01:55<41:13,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 108/2436 [01:56<41:15,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 109/2436 [01:57<41:16,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 110/2436 [01:58<41:11,  1.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   5%|▍         | 111/2436 [02:00<40:59,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 112/2436 [02:01<40:55,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 113/2436 [02:02<41:01,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 114/2436 [02:03<41:01,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 115/2436 [02:04<41:00,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 116/2436 [02:05<40:56,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 117/2436 [02:06<40:59,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 118/2436 [02:07<40:55,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 119/2436 [02:08<40:46,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 120/2436 [02:09<40:44,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 121/2436 [02:10<40:51,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 122/2436 [02:11<40:46,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 123/2436 [02:12<40:42,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 124/2436 [02:13<40:42,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 125/2436 [02:14<40:48,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 126/2436 [02:15<40:42,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 127/2436 [02:16<40:37,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 128/2436 [02:18<40:37,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 129/2436 [02:19<40:41,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 130/2436 [02:20<40:34,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 131/2436 [02:21<40:39,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 132/2436 [02:22<40:39,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 133/2436 [02:23<40:42,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 134/2436 [02:24<40:38,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 135/2436 [02:25<40:36,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 136/2436 [02:26<40:29,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 137/2436 [02:27<40:35,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 138/2436 [02:28<40:29,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 139/2436 [02:29<40:23,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 140/2436 [02:30<40:17,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 141/2436 [02:31<40:24,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 142/2436 [02:32<40:21,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 143/2436 [02:33<40:18,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 144/2436 [02:34<40:17,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 145/2436 [02:35<40:23,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 146/2436 [02:37<40:20,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 147/2436 [02:38<40:17,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 148/2436 [02:39<40:17,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 149/2436 [02:40<40:21,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 150/2436 [02:41<40:11,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 151/2436 [02:42<40:05,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 152/2436 [02:43<40:03,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 153/2436 [02:44<40:14,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 154/2436 [02:45<40:10,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 155/2436 [02:46<40:07,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 156/2436 [02:47<40:04,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 157/2436 [02:48<40:05,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 158/2436 [02:49<40:07,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 159/2436 [02:50<40:02,  1.05s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 160/2436 [02:51<40:02,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 161/2436 [02:52<40:12,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 162/2436 [02:53<40:05,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 163/2436 [02:54<40:00,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 164/2436 [02:56<39:57,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 165/2436 [02:57<40:07,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 166/2436 [02:58<40:00,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 167/2436 [02:59<40:01,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 168/2436 [03:00<39:56,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 169/2436 [03:01<40:07,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 170/2436 [03:02<39:57,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 171/2436 [03:03<39:49,  1.05s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 172/2436 [03:04<39:48,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 173/2436 [03:05<39:58,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 174/2436 [03:06<39:50,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 175/2436 [03:07<39:42,  1.05s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 176/2436 [03:08<39:43,  1.05s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 177/2436 [03:09<39:54,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 178/2436 [03:10<39:50,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 179/2436 [03:11<39:51,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 180/2436 [03:12<39:49,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 181/2436 [03:14<39:53,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 182/2436 [03:15<39:41,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 183/2436 [03:16<39:34,  1.05s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 184/2436 [03:17<39:32,  1.05s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 185/2436 [03:18<39:40,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 186/2436 [03:19<39:34,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 187/2436 [03:20<39:34,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 188/2436 [03:21<39:35,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 189/2436 [03:22<39:44,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 190/2436 [03:23<39:36,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 191/2436 [03:24<39:37,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 192/2436 [03:25<39:30,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 193/2436 [03:26<39:33,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 194/2436 [03:27<39:25,  1.05s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 195/2436 [03:28<39:24,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 196/2436 [03:29<39:19,  1.05s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 197/2436 [03:30<39:28,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 198/2436 [03:31<39:22,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 199/2436 [03:33<39:21,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 200/2436 [03:34<39:22,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 201/2436 [03:35<39:27,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 202/2436 [03:36<39:23,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 203/2436 [03:37<39:17,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 204/2436 [03:38<39:16,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 205/2436 [03:39<39:24,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 206/2436 [03:40<39:13,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 207/2436 [03:41<39:10,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 208/2436 [03:42<39:05,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 209/2436 [03:43<39:14,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 210/2436 [03:44<39:08,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 211/2436 [03:45<39:06,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 212/2436 [03:46<39:02,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 213/2436 [03:47<39:06,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 214/2436 [03:48<38:59,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 215/2436 [03:49<38:56,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 216/2436 [03:50<38:55,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 217/2436 [03:52<39:10,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 218/2436 [03:53<39:05,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 219/2436 [03:54<39:00,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 220/2436 [03:55<38:58,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 221/2436 [03:56<39:04,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 222/2436 [03:57<38:58,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 223/2436 [03:58<38:55,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 224/2436 [03:59<38:49,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 225/2436 [04:00<38:57,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 226/2436 [04:01<38:49,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 227/2436 [04:02<38:42,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 228/2436 [04:03<38:42,  1.05s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 229/2436 [04:04<38:50,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 230/2436 [04:05<38:51,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 231/2436 [04:06<38:46,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 232/2436 [04:07<38:41,  1.05s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 233/2436 [04:08<38:48,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 234/2436 [04:09<38:43,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 235/2436 [04:11<38:38,  1.05s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 236/2436 [04:12<38:39,  1.05s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  10%|▉         | 237/2436 [04:13<38:45,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 238/2436 [04:14<38:38,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 239/2436 [04:15<38:39,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 240/2436 [04:16<38:38,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 241/2436 [04:17<38:44,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 242/2436 [04:18<38:45,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 243/2436 [04:19<38:37,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 244/2436 [04:20<38:33,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 245/2436 [04:21<38:43,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 246/2436 [04:22<38:40,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 247/2436 [04:23<38:33,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 248/2436 [04:24<38:31,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 249/2436 [04:25<38:40,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 250/2436 [04:26<38:34,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 251/2436 [04:27<38:28,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 252/2436 [04:29<38:20,  1.05s/it]\u001b[A\n",
      "Iteration:  10%|█         | 253/2436 [04:30<38:27,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|█         | 254/2436 [04:31<38:16,  1.05s/it]\u001b[A\n",
      "Iteration:  10%|█         | 255/2436 [04:32<38:19,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 256/2436 [04:33<38:26,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 257/2436 [04:34<38:31,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 258/2436 [04:35<38:26,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 259/2436 [04:36<38:20,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 260/2436 [04:37<38:14,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 261/2436 [04:38<38:19,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 262/2436 [04:39<38:15,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 263/2436 [04:40<38:18,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 264/2436 [04:41<38:17,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 265/2436 [04:42<38:20,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 266/2436 [04:43<38:09,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 267/2436 [04:44<38:01,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 268/2436 [04:45<37:58,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 269/2436 [04:46<38:04,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 270/2436 [04:48<38:00,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 271/2436 [04:49<37:54,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 272/2436 [04:50<38:01,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 273/2436 [04:51<38:14,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 274/2436 [04:52<38:01,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 275/2436 [04:53<37:54,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 276/2436 [04:54<37:54,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 277/2436 [04:55<38:02,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 278/2436 [04:56<38:01,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 279/2436 [04:57<37:57,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 280/2436 [04:58<37:58,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 281/2436 [04:59<38:02,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 282/2436 [05:00<37:55,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 283/2436 [05:01<37:49,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 284/2436 [05:02<37:45,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 285/2436 [05:03<37:52,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 286/2436 [05:04<37:47,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 287/2436 [05:05<37:51,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 288/2436 [05:07<37:44,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 289/2436 [05:08<37:51,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 290/2436 [05:09<37:48,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 291/2436 [05:10<37:45,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 292/2436 [05:11<37:46,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 293/2436 [05:12<37:49,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 294/2436 [05:13<37:44,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 295/2436 [05:14<37:35,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 296/2436 [05:15<37:33,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 297/2436 [05:16<37:40,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 298/2436 [05:17<37:36,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 299/2436 [05:18<37:26,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 300/2436 [05:19<37:30,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 301/2436 [05:20<37:36,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 302/2436 [05:21<37:30,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 303/2436 [05:22<37:23,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 304/2436 [05:23<37:23,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 305/2436 [05:24<37:31,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 306/2436 [05:26<37:24,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 307/2436 [05:27<37:20,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 308/2436 [05:28<37:22,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 309/2436 [05:29<37:29,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 310/2436 [05:30<37:23,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 311/2436 [05:31<37:14,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 312/2436 [05:32<37:17,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 313/2436 [05:33<37:26,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 314/2436 [05:34<37:18,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 315/2436 [05:35<37:16,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 316/2436 [05:36<37:16,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 317/2436 [05:37<37:21,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 318/2436 [05:38<37:14,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 319/2436 [05:39<37:22,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 320/2436 [05:40<37:16,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 321/2436 [05:41<37:17,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 322/2436 [05:42<37:09,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 323/2436 [05:43<37:08,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 324/2436 [05:45<37:11,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 325/2436 [05:46<37:16,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 326/2436 [05:47<37:15,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 327/2436 [05:48<37:09,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 328/2436 [05:49<37:02,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 329/2436 [05:50<37:13,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 330/2436 [05:51<37:08,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 331/2436 [05:52<37:06,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 332/2436 [05:53<37:00,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 333/2436 [05:54<37:05,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 334/2436 [05:55<36:59,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 335/2436 [05:56<36:56,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 336/2436 [05:57<36:51,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 337/2436 [05:58<36:58,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 338/2436 [05:59<36:54,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 339/2436 [06:00<36:46,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 340/2436 [06:01<36:47,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 341/2436 [06:02<36:54,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 342/2436 [06:04<36:47,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 343/2436 [06:05<36:42,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 344/2436 [06:06<36:39,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 345/2436 [06:07<36:47,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 346/2436 [06:08<36:41,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 347/2436 [06:09<36:37,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 348/2436 [06:10<36:39,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 349/2436 [06:11<36:47,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 350/2436 [06:12<36:44,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 351/2436 [06:13<36:43,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 352/2436 [06:14<36:44,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 353/2436 [06:15<36:47,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 354/2436 [06:16<36:53,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 355/2436 [06:17<36:48,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 356/2436 [06:18<36:39,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 357/2436 [06:19<36:46,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 358/2436 [06:20<36:38,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 359/2436 [06:21<36:33,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 360/2436 [06:23<36:31,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 361/2436 [06:24<36:37,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 362/2436 [06:25<36:28,  1.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  15%|█▍        | 363/2436 [06:26<36:18,  1.05s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 364/2436 [06:27<36:23,  1.05s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 365/2436 [06:28<36:31,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 366/2436 [06:29<36:28,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 367/2436 [06:30<36:27,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 368/2436 [06:31<36:22,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 369/2436 [06:32<36:27,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 370/2436 [06:33<36:23,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 371/2436 [06:34<36:20,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 372/2436 [06:35<36:19,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 373/2436 [06:36<36:25,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 374/2436 [06:37<36:18,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 375/2436 [06:38<36:17,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 376/2436 [06:39<36:12,  1.05s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 377/2436 [06:41<36:19,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 378/2436 [06:42<36:12,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 379/2436 [06:43<36:09,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 380/2436 [06:44<36:06,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 381/2436 [06:45<36:16,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 382/2436 [06:46<36:12,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 383/2436 [06:47<36:09,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 384/2436 [06:48<36:03,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 385/2436 [06:49<36:09,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 386/2436 [06:50<36:04,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 387/2436 [06:51<35:59,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 388/2436 [06:52<36:00,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 389/2436 [06:53<36:06,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 390/2436 [06:54<35:58,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 391/2436 [06:55<35:53,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 392/2436 [06:56<35:50,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 393/2436 [06:57<35:59,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 394/2436 [06:58<35:59,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 395/2436 [07:00<35:50,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 396/2436 [07:01<35:51,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 397/2436 [07:02<35:58,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 398/2436 [07:03<35:51,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 399/2436 [07:04<35:48,  1.05s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 400/2436 [07:05<35:48,  1.06s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 401/2436 [07:06<35:59,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 402/2436 [07:07<35:51,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 403/2436 [07:08<35:46,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 404/2436 [07:09<35:44,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 405/2436 [07:10<35:52,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 406/2436 [07:11<35:50,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 407/2436 [07:12<35:42,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 408/2436 [07:13<35:38,  1.05s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 409/2436 [07:14<35:45,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 410/2436 [07:15<35:41,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 411/2436 [07:16<35:34,  1.05s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 412/2436 [07:17<35:31,  1.05s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 413/2436 [07:19<35:38,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 414/2436 [07:20<35:37,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 415/2436 [07:21<35:36,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 416/2436 [07:22<35:37,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 417/2436 [07:23<35:42,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 418/2436 [07:24<35:37,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 419/2436 [07:25<35:34,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 420/2436 [07:26<35:31,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 421/2436 [07:27<35:36,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 422/2436 [07:28<35:30,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 423/2436 [07:29<35:26,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 424/2436 [07:30<35:27,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 425/2436 [07:31<35:32,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 426/2436 [07:32<35:25,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 427/2436 [07:33<35:19,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 428/2436 [07:34<35:23,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 429/2436 [07:35<35:25,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 430/2436 [07:37<35:14,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 431/2436 [07:38<35:11,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 432/2436 [07:39<35:09,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 433/2436 [07:40<35:16,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 434/2436 [07:41<35:11,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 435/2436 [07:42<35:10,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 436/2436 [07:43<35:08,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 437/2436 [07:44<35:13,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 438/2436 [07:45<35:09,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 439/2436 [07:46<35:04,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 440/2436 [07:47<35:01,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 441/2436 [07:48<35:11,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 442/2436 [07:49<35:10,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 443/2436 [07:50<35:10,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 444/2436 [07:51<35:04,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 445/2436 [07:52<35:12,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 446/2436 [07:53<35:07,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 447/2436 [07:54<34:59,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 448/2436 [07:56<34:55,  1.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 449/2436 [07:57<35:02,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 450/2436 [07:58<35:04,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 451/2436 [07:59<34:58,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 452/2436 [08:00<34:58,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 453/2436 [08:01<35:01,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 454/2436 [08:02<34:54,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 455/2436 [08:03<34:46,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 456/2436 [08:04<34:48,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 457/2436 [08:05<34:57,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 458/2436 [08:06<34:45,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 459/2436 [08:07<34:41,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 460/2436 [08:08<34:41,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 461/2436 [08:09<34:50,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 462/2436 [08:10<34:54,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 463/2436 [08:11<34:48,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 464/2436 [08:12<34:44,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 465/2436 [08:14<34:46,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 466/2436 [08:15<34:40,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 467/2436 [08:16<34:35,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 468/2436 [08:17<34:34,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 469/2436 [08:18<34:41,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 470/2436 [08:19<34:36,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 471/2436 [08:20<34:32,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 472/2436 [08:21<34:27,  1.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 473/2436 [08:22<34:34,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 474/2436 [08:23<34:31,  1.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 475/2436 [08:24<34:23,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 476/2436 [08:25<34:25,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 477/2436 [08:26<34:31,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 478/2436 [08:27<34:25,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 479/2436 [08:28<34:21,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 480/2436 [08:29<34:20,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 481/2436 [08:30<34:28,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 482/2436 [08:31<34:21,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 483/2436 [08:32<34:15,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 484/2436 [08:34<34:16,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 485/2436 [08:35<34:22,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 486/2436 [08:36<34:19,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 487/2436 [08:37<34:17,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|██        | 488/2436 [08:38<34:15,  1.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 489/2436 [08:39<34:23,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|██        | 490/2436 [08:40<34:16,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|██        | 491/2436 [08:41<34:10,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|██        | 492/2436 [08:42<34:10,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|██        | 493/2436 [08:43<34:12,  1.06s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "if args.do_train:\n",
    "    train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n",
    "    global_step, tr_loss, best_steps = train(args, train_dataset, model, tokenizer)\n",
    "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "    # They can then be reloaded using `from_pretrained()`\n",
    "    model_to_save = (\n",
    "        model.module if hasattr(model, \"module\") else model\n",
    "    )  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = model_class.from_pretrained(args.output_dir)\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
    "    model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "results = {}\n",
    "model_outputs = None\n",
    "if args.do_eval and args.local_rank in [-1, 0]:\n",
    "    if not args.do_train:\n",
    "        args.output_dir = args.model_name_or_path\n",
    "    checkpoints = [args.output_dir]\n",
    "    if args.eval_all_checkpoints:\n",
    "        checkpoints = list(\n",
    "            os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "        )\n",
    "        logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "    for checkpoint in checkpoints:\n",
    "        global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "        prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "        model = model_class.from_pretrained(checkpoint)\n",
    "        model.to(args.device)\n",
    "        result = evaluate(args, model, tokenizer, prefix=prefix)\n",
    "        result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "        results.update(result)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_nrj_env",
   "language": "python",
   "name": "transformers_nrj_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
